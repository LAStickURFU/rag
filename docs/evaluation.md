# Оценка качества RAG-системы

В этом документе описаны инструменты и методики для оценки качества работы RAG-системы, которые помогают проанализировать эффективность поиска, достоверность и релевантность генерируемых ответов.

## Обзор модуля оценки

Модуль оценки (`evaluation/`) позволяет проводить комплексную проверку всех ключевых аспектов RAG-системы:

```
evaluation/
├── evaluate.py               # Главный скрипт для запуска оценки
├── metrics.py                # Расчёт метрик (локальных и RAGAS)
├── test_dataset.json         # Набор примеров с вопросами, эталонными ответами и контекстом
├── results/
│   ├── eval_results.json             # Детальные метрики на каждый пример
│   ├── eval_results_summary.csv      # Сводная таблица со средними значениями
│   ├── eval_results_metrics.svg/png  # Гистограмма средних значений
│   ├── eval_results_boxplot.svg/png  # Boxplot распределения
│   └── eval_results_llm_scatter.svg/png # Scatter-график GPT-оценок
```

## Метрики оценки и их интерпретация

### 1. Качество ответа

Метрики для оценки соответствия сгенерированного ответа эталонному:

| Метрика | Описание | Интерпретация |
| ------- | -------- | ------------- |
| `rouge_l` | Мера перекрытия токенов между ответами | ≥ 0.6 — хорошее совпадение по форме |
| `bertscore` | Семантическое сходство на уровне смыслов | ≥ 0.85 — высокое семантическое совпадение |

### 2. Метрики поиска (retrieval)

Оценивают, насколько хорошо система находит релевантные фрагменты:

| Метрика | Описание | Интерпретация |
| ------- | -------- | ------------- |
| `context_precision_manual` | Доля извлечённых чанков, которые действительно релевантны | Высокая точность → меньше шумных чанков |
| `context_recall_manual` | Доля релевантных чанков, которые удалось найти | Высокий recall → система извлекла все важные части |

*Примечание*: Обе метрики рассчитываются на основе ожидаемого контекста из `test_dataset.json`.

### 3. Достоверность ответа (faithfulness)

Оценивают, насколько ответ опирается на предоставленный контекст:

| Метрика | Описание | Интерпретация |
| ------- | -------- | ------------- |
| `faithfulness_semantic` | Сходство между ответом и суммарным контекстом (косинусное сходство) | ≥ 0.8 считается хорошим результатом |
| `ragas_faithfulness` | Встроенная метрика RAGAS для проверки достоверности | Значение от 0 до 1, выше 0.8 - хорошо |

### 4. RAGAS-метрики

[RAGAS](https://github.com/explodinggradients/ragas) - это библиотека для автоматической оценки RAG-систем:

| Метрика | Описание | Интерпретация |
| ------- | -------- | ------------- |
| `ragas_answer_relevance` | Насколько ответ отвечает на вопрос | ≥ 0.8 - хороший результат |
| `ragas_context_precision` | Аналог `context_precision_manual`, но на основе семантики | ≥ 0.8 - хороший результат |
| `ragas_context_recall` | Аналог `context_recall_manual`, но на основе семантики | ≥ 0.8 - хороший результат |

### 5. LLM-оценка (экспертная)

Использует GPT-3.5 для оценки двух ключевых аспектов ответа:

#### `llm_relevance`

**Промпт**: "Насколько данный ответ релевантен вопросу, независимо от контекста? Ответь одним числом от 0 до 1."

**Что измеряет**: логичность и соответствие ответа формулировке вопроса
- 1.0 — ответ полностью уместен и логичен
- 0.0 — ответ не соответствует вопросу

#### `llm_faithfulness`

**Промпт**: "Насколько данный ответ соответствует только предоставленному контексту, без добавления лишней информации?"

**Что измеряет**: опирается ли ответ строго на данные, извлечённые системой
- 1.0 — ответ строго основан на контексте
- < 0.5 — вероятна галлюцинация

Важно различать эти два аспекта:
- Высокий `llm_relevance` + низкий `llm_faithfulness` → ответ звучит логично, но не основан на фактах
- Низкий `llm_relevance` + высокий `llm_faithfulness` → ответ верен, но не по теме

## Запуск оценки

### Подготовка

1. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

2. Установите API-ключ OpenAI (для LLM-оценки):
   ```bash
   export OPENAI_API_KEY=your_openai_key
   ```

### Запуск оценки

```bash
python evaluation/evaluate.py --dataset evaluation/test_dataset.json
```

#### Опции запуска:

- `--dataset`: путь к JSON-файлу с тестовыми примерами
- `--api-url`: URL API системы для оценки (по умолчанию: http://localhost:8000)
- `--exact-match`: использовать строгое сравнение чанков (по умолчанию используется семантическое)
- `--output-dir`: директория для сохранения результатов

### Формат тестового набора данных

Пример записи в `test_dataset.json`:

```json
[
  {
    "question": "Какие преимущества у RAG по сравнению с обычным LLM?",
    "expected_answer": "RAG имеет ряд преимуществ: основывает ответы на проверенных источниках, снижает галлюцинации, обеспечивает актуальность информации и позволяет цитировать источники.",
    "expected_context": [
      "RAG (Retrieval-Augmented Generation) улучшает работу языковых моделей через поиск релевантной информации.",
      "Основные преимущества RAG: ответы на основе фактов, меньше галлюцинаций, актуальность данных, прозрачность источников."
    ]
  }
]
```

## Визуализация результатов

Скрипт создаёт следующие визуализации в директории `results/`:

1. **Гистограмма средних значений** (`*_metrics.png/svg`):
   - Средние значения всех метрик для быстрой оценки общей эффективности

2. **Boxplot распределения** (`*_boxplot.png/svg`):
   - Распределение значений метрик по отдельным примерам
   - Показывает разброс, медиану, квартили и выбросы

3. **Scatter-график LLM-оценок** (`*_llm_scatter.png/svg`):
   - Визуализирует соотношение релевантности и достоверности ответов
   - Помогает выявить ответы, которые звучат хорошо, но содержат галлюцинации

## Интерпретация результатов

### Идеальный RAG

Характеристики хорошо работающей RAG-системы:

- **Высокие метрики поиска**: context_precision и context_recall > 0.8
- **Высокая достоверность**: faithfulness > 0.85
- **Высокая релевантность**: answer_relevance > 0.9
- **Согласованность LLM-оценок**: llm_relevance и llm_faithfulness > 0.85

### Типичные проблемы и их диагностика

| Проблема | Симптомы в метриках | Возможные решения |
| -------- | ------------------- | ----------------- |
| Нерелевантный поиск | Низкий context_precision | Улучшение эмбеддингов, настройка k, переранжирование |
| Неполный поиск | Низкий context_recall | Увеличение k, настройка разбиения на чанки |
| Галлюцинации | Низкий faithfulness, высокий relevance | Улучшение промпта, уменьшение температуры генерации |
| Плохое качество ответа | Низкий relevance, нормальный faithfulness | Настройка инструкций для LLM, улучшение формата контекста |

## Регулярное тестирование

Рекомендуется проводить оценку в следующих случаях:

1. При изменении ключевых параметров системы:
   - Размер чанков и стратегия чанкинга
   - Модель эмбеддингов
   - Настройки гибридного поиска
   - Темплейт промпта

2. При добавлении новых типов данных или документов

3. При обновлении компонентов системы (библиотек, моделей)

## Дальнейшее развитие системы оценки

Планируемые улучшения:

- Интеграция с CI/CD для автоматического тестирования
- Добавление тестов на различных языках
- Расширение типов вопросов (фактоидные, многоходовые, рассуждения)
- Оценка с привлечением человеческих экспертов
- Сравнительное тестирование против других RAG-систем и коммерческих решений 