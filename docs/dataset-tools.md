# Инструменты для работы с данными

В этом документе описаны инструменты для сбора, обработки и анализа данных для RAG-системы, расположенные в директории `dataset_script/`.

## Обзор компонентов

```
dataset_script/
├── parser.py               # Скрипт для сбора данных с веб-сайтов
├── rag_dataset_analysis.py # Анализ текстового корпуса для RAG
├── sitemap.xml             # Карта сайта с URL для сбора данных (17720 ссылок)
├── Readme.md               # Документация по использованию скриптов
├── requirements.txt        # Зависимости для установки
├── text/                   # Каталог с извлеченными текстами (2712 файлов)
└── output/                 # Результаты анализа (статистика, графики, CSV/JSON)
```

## 1. Сбор данных (`parser.py`)

Инструмент для парсинга веб-сайтов и сохранения текстового содержимого страниц.

### Возможности

- Автоматическое извлечение URL из файла sitemap.xml
- Управление браузером через Playwright для обхода защиты от парсинга
- Асинхронная обработка с ограничением одновременных запросов
- Фильтрация контента с удалением навигации, рекламы и других нерелевантных элементов
- Система повторных попыток при ошибках и логирование прогресса

### Как использовать

1. Установите зависимости:
   ```bash
   pip install -r dataset_script/requirements.txt
   ```

2. Запустите парсер, указав путь к файлу sitemap.xml:
   ```bash
   python -m dataset_script.parser --sitemap dataset_script/sitemap.xml
   ```

3. Дополнительные опции:
   ```bash
   python -m dataset_script.parser --sitemap dataset_script/sitemap.xml --limit 100 --concurrency 5 --output-dir ./dataset_script/text
   ```

   - `--limit` - максимальное количество страниц для обработки
   - `--concurrency` - количество одновременных запросов
   - `--output-dir` - директория для сохранения результатов

### Структура выходных данных

Для каждой обработанной страницы создается отдельный текстовый файл в директории `text/`. Имя файла формируется на основе URL-адреса страницы, с заменой специальных символов.

Пример содержимого файла:
```
# Название страницы (заголовок H1)

Основной текст страницы, разбитый на абзацы.

## Подзаголовок (H2)

Текст раздела...
```

## 2. Анализ данных (`rag_dataset_analysis.py`)

Инструмент для анализа собранного текстового корпуса и оценки его пригодности для использования в RAG-системе.

### Возможности

- Подсчет основных статистических показателей (количество слов, предложений, символов)
- Определение языка документов и выявление дубликатов
- Расчет метрик качества для RAG:
  - Type-Token Ratio (TTR) - отношение количества уникальных слов к общему числу
  - Noise ratio - доля шума (специальные символы, цифры, др.)
  - Paragraph structure - анализ структуры абзацев
- Симуляция разбиения на чанки разных размеров (256, 512, 1024 токена)
- Генерация визуализаций (облака слов, распределения длин)

### Как использовать

1. Установите зависимости:
   ```bash
   pip install -r dataset_script/requirements.txt
   ```

2. Запустите анализ:
   ```bash
   python -m dataset_script.rag_dataset_analysis --input-dir ./dataset_script/text --output-dir ./dataset_script/output
   ```

3. Дополнительные опции:
   ```bash
   python -m dataset_script.rag_dataset_analysis --input-dir ./dataset_script/text --output-dir ./dataset_script/output --visualize --sample-size 100
   ```

   - `--visualize` - создание визуализаций
   - `--sample-size` - размер выборки для анализа (0 = все файлы)
   - `--chunk-sizes` - размеры чанков для симуляции (по умолчанию "256,512,1024")

### Результаты анализа

В директории `output/` создаются файлы с результатами анализа:

- `dataset_stats.csv` и `dataset_stats.json` - статистика по каждому документу
- `dataset_summary.json` - сводная статистика по всему корпусу
- `token_distribution.png` - гистограмма распределения длин документов в токенах
- `wordcloud.png` - облако слов на основе всего корпуса
- `chunking_simulation.json` - результаты симуляции чанкинга

## 3. Основные результаты анализа

На основе проведенного анализа корпуса из 2712 текстовых документов выявлены следующие особенности:

### Общие характеристики

- **Размер корпуса**: 2712 документов на русском языке
- **Объем данных**: 1.5 млн слов, 11 млн символов
- **Качество данных**: 14 дубликатов (0.5%), среднее TTR 0.70

### Распределение размеров

- 27% документов (733) содержат менее 100 слов
- 39% документов (1063) содержат 100-299 слов
- 34% документов (916) содержат 300+ слов

### Эффективность чанкинга

- **Чанки по 256 токенов**: в среднем 2.74 чанка на документ
- **Чанки по 512 токенов**: в среднем 1.77 чанка на документ
- **Чанки по 1024 токена**: в среднем 1.30 чанка на документ

## 4. Применение в проекте

Данные инструменты можно использовать для:

### Подготовка базы знаний

1. Идентифицируйте источники информации (веб-сайты, документация, статьи)
2. Создайте или экспортируйте sitemap.xml для сайтов
3. Соберите тексты с помощью `parser.py`
4. Проведите анализ с помощью `rag_dataset_analysis.py`
5. Оптимизируйте параметры чанкинга в RAG-системе на основе результатов анализа

### Оценка качества корпуса

Используйте метрики из анализа для оценки пригодности корпуса:
- TTR > 0.6 указывает на лексическое разнообразие и информативность
- Низкий процент дубликатов (< 1%) говорит о чистоте данных
- Оптимальное распределение длин документов снижает необходимость в сложных стратегиях чанкинга

### Эксперименты с размерами чанков

На основе симуляции чанкинга определите оптимальный размер:
- Меньшие чанки (256 токенов) дают более точный поиск, но увеличивают число индексируемых единиц
- Более крупные чанки (1024 токена) уменьшают размер индекса, но могут снизить точность поиска

## 5. Интеграция с основной системой

### Загрузка подготовленных данных

1. Подготовьте данные с помощью парсера
2. Проанализируйте и отфильтруйте плохие документы
3. Загрузите тексты через API RAG-системы:

```python
import requests
import os

def upload_documents(api_url, text_dir, token):
    files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]
    
    for file in files:
        filepath = os.path.join(text_dir, file)
        
        with open(filepath, 'rb') as f:
            title = os.path.splitext(file)[0]
            response = requests.post(
                f"{api_url}/documents/upload",
                files={"files": (file, f)},
                data={"titles": title},
                headers={"Authorization": f"Bearer {token}"}
            )
            
            if response.status_code == 200:
                print(f"Uploaded {file}")
            else:
                print(f"Failed to upload {file}: {response.text}")

# Пример использования
upload_documents(
    api_url="http://localhost:8000",
    text_dir="dataset_script/text",
    token="your_auth_token"
)
```

### Настройка параметров чанкинга

На основе результатов анализа настройте параметры `.env`:

```ini
# Для коллекции с преобладанием коротких документов
CHUNK_SIZE=256
CHUNK_OVERLAP=50
CHUNKING_MODE=character

# Для коллекции с преобладанием длинных документов
CHUNK_SIZE=512
CHUNK_OVERLAP=100
CHUNKING_MODE=hierarchical
```

## 6. Рекомендации по оптимизации корпуса

- **Для коротких документов** (< 100 слов) рекомендуется символьный чанкинг с маленьким размером чанка (256 токенов)
- **Для средних документов** (100-500 слов) оптимален токеновый чанкинг с размером 512 токенов
- **Для длинных документов** (> 500 слов) рекомендуется иерархический чанкинг с учетом структуры документа
- **Для смешанных корпусов** эффективен семантический чанкинг, разделяющий текст по смысловым блокам

## 7. Дальнейшее развитие инструментов

Планируемые улучшения:
- Поддержка многоязычных корпусов
- Автоматическое определение оптимального размера чанка
- Интерактивная визуализация результатов анализа
- Интеграция с популярными источниками данных (API, RSS, базы знаний) 