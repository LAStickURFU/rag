# RAG - Retrieval-Augmented Generation

Система генерации ответов на вопросы, основанная на векторном поиске релевантной информации из загруженных документов.

## Описание проекта

Данная система реализует технологию RAG (Retrieval-Augmented Generation), которая позволяет улучшить ответы языковых моделей (LLM) путем предоставления им релевантного контекста, найденного в базе знаний. Основные компоненты системы:

1. **Бэкенд (FastAPI)**: REST API для обработки документов, векторизации, поиска и генерации ответов
2. **Vectorstore (Qdrant)**: векторная база данных для хранения эмбеддингов текстовых фрагментов
3. **Языковая модель (Ollama)**: локальная модель для генерации ответов
4. **Фронтенд (React)**: пользовательский интерфейс для взаимодействия с системой

## Функциональные возможности

- Загрузка и индексация документов различных форматов (PDF, DOCX, TXT, MD, JSON и др.)
- Разделение документов на фрагменты (чанки) с использованием символьного или токенного чанкера
- Векторизация текстовых фрагментов с использованием моделей embeddings (например, E5)
- Гибридный поиск (dense + sparse) с поддержкой переранжирования
- Интерактивный чат с возможностью просмотра найденных источников
- Система пользовательской авторизации и хранения истории запросов
- Отслеживание прогресса обработки документов в реальном времени
- Автоматическое обновление страницы при обработке документов

## Требования

- Python 3.10+
- Ollama (локальная языковая модель)
- PostgreSQL
- Node.js 14+ (для фронтенда)
- Docker и Docker Compose (опционально)

## Установка и запуск

### Вариант 1: Запуск с помощью Docker Compose

1. Клонировать репозиторий
2. Создать файл `.env` на основе `.env.example`:
   ```bash
   cp .env.example .env
   ```
3. Отредактировать `.env` под свои нужды
4. Запустить с помощью Docker Compose:
   ```bash
   docker-compose up -d
   ```

### Вариант 2: Локальный запуск

#### Бэкенд

1. Установить зависимости:
   ```bash
   pip install -r requirements.txt
   ```
2. Установить и запустить Ollama с нужной моделью:
   ```bash
   # Установить Ollama с официального сайта https://ollama.com/download
   # Запустить модель (например, Mistral 7B Instruct)
   ollama run mistral:7b-instruct
   ```
3. Запустить бэкенд:
   ```bash
   uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
   ```

#### Фронтенд

1. Перейти в директорию frontend:
   ```bash
   cd frontend
   ```
2. Установить зависимости:
   ```bash
   npm install
   ```
3. Запустить фронтенд:
   ```bash
   npm start
   ```

## Конфигурация

Основные настройки системы находятся в файле `.env`. Некоторые важные параметры:

- `EMBEDDING_MODEL`: модель для создания векторных представлений текста (по умолчанию: intfloat/multilingual-e5-large)
- `OLLAMA_MODEL`: модель Ollama для генерации ответов (по умолчанию: mistral:7b-instruct)
- `CHUNK_SIZE`: размер текстовых фрагментов в символах (по умолчанию: 400)
- `CHUNK_OVERLAP`: перекрытие между фрагментами (по умолчанию: 100)
- `USE_HYBRID_SEARCH`: использовать гибридный поиск (по умолчанию: true)
- `USE_RERANKER`: использовать переранжирование (по умолчанию: true)
- `USE_TOKEN_CHUNKER`: использовать токеновый чанкер вместо символьного (по умолчанию: true)

## Архитектура проекта

```
rag/
├── app/                  # Основной код бэкенда
│   ├── chunking/         # Модули для разделения текста на фрагменты
│   ├── retrieval/        # Компоненты для векторного поиска
│   ├── utils/            # Вспомогательные функции
│   ├── config.py         # Конфигурация приложения
│   ├── database.py       # Работа с базой данных
│   ├── logging_config.py # Настройка логирования
│   ├── main.py           # Основной файл FastAPI приложения
│   ├── models.py         # Модели базы данных (ORM)
│   ├── ollama_client.py  # Клиент для взаимодействия с Ollama
│   ├── rag.py            # Основной сервис RAG
│   └── schemas.py        # Pydantic модели для FastAPI
├── data/                 # Данные для приложения
├── frontend/             # React фронтенд
│   ├── public/           # Статические файлы
│   └── src/              # Исходный код фронтенда
├── indexes/              # Директория для хранения индексов
├── logs/                 # Логи приложения
├── scripts/              # Вспомогательные скрипты
├── tests/                # Тесты
├── .env.example          # Пример конфигурации окружения
├── Dockerfile            # Конфигурация Docker образа
├── docker-compose.yml    # Конфигурация Docker Compose
└── requirements.txt      # Python зависимости
```

## API Endpoints

### Аутентификация
- `POST /token` - получение токена доступа
- `POST /register` - регистрация нового пользователя

### Работа с документами
- `POST /documents/upload` - загрузка новых документов
- `GET /documents` - получение списка загруженных документов
- `DELETE /documents/{document_id}` - удаление документа
- `GET /documents/{document_id}` - получение информации о документе
- `POST /documents/reindex` - переиндексация документов
- `POST /index/clear` - очистка индекса

### Чат и генерация ответов
- `POST /ask` - запрос с использованием RAG
- `POST /direct-ask` - прямой запрос к модели без RAG
- `GET /chats` - получение истории чатов
- `DELETE /chats/clear` - очистка истории чатов

### Настройки модели
- `GET /model/settings` - получение настроек модели
- `POST /model/settings` - обновление настроек модели

## Как использовать

1. **Регистрация и вход**: зарегистрируйтесь и войдите в систему через фронтенд
2. **Загрузка документов**: загрузите документы через интерфейс загрузки
3. **Мониторинг обработки**: отслеживайте прогресс обработки документов в реальном времени
   - Просматривайте текущие статусы документов: загрузка, извлечение текста, разбиение, создание эмбеддингов, индексация
   - Наблюдайте за общим индикатором прогресса для всех документов
   - Страница автоматически обновляется каждые 2 секунды во время обработки
4. **Задавание вопросов**: задавайте вопросы в чат-интерфейсе. Система найдет релевантную информацию в загруженных документах и сгенерирует ответ
5. **Настройка параметров**: при необходимости настройте параметры модели в разделе настроек

## Жизненный цикл обработки документов

Система отображает следующие статусы обработки документов:

1. **uploaded**: документ загружен, ожидает обработки (10% прогресса)
2. **processing**: извлечение текста из документа (25% прогресса)
3. **chunking**: разбиение текста на фрагменты (50% прогресса)
4. **embedding**: создание векторных представлений фрагментов (75% прогресса)
5. **indexing**: добавление векторов в индекс (90% прогресса)
6. **indexed**: документ успешно обработан и проиндексирован (100% прогресса)
7. **reindexing**: документ находится в процессе переиндексации
8. **error**: ошибка при обработке документа

## Разработка и тестирование

Проект включает тесты, которые можно запустить следующим образом:

```bash
# Запуск тестов бэкенда
pytest

# Запуск тестов фронтенда
cd frontend
npm test
```

## Лицензия

[MIT](LICENSE) 