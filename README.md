# RAG - Retrieval-Augmented Generation

Система генерации ответов на вопросы, основанная на векторном поиске релевантной информации из загруженных документов.

## Описание проекта

Данная система реализует технологию RAG (Retrieval-Augmented Generation), которая позволяет улучшить ответы языковых моделей (LLM) путем предоставления им релевантного контекста, найденного в базе знаний. Основные компоненты системы:

1. **Бэкенд (FastAPI)**: REST API для обработки документов, векторизации, поиска и генерации ответов
2. **Vectorstore (Qdrant)**: векторная база данных для хранения эмбеддингов текстовых фрагментов
3. **Языковая модель (Ollama)**: локальная модель для генерации ответов
4. **Фронтенд (React)**: пользовательский интерфейс для взаимодействия с системой

## Функциональные возможности

- Загрузка и индексация документов различных форматов (PDF, DOCX, TXT, MD, JSON и др.)
- Разделение документов на фрагменты (чанки) с использованием различных стратегий (символьный, токенный, иерархический)
- Векторизация текстовых фрагментов с использованием моделей embeddings (например, E5)
- Гибридный поиск (dense + sparse) с поддержкой переранжирования
- Интерактивный чат с возможностью просмотра найденных источников
- Система пользовательской авторизации и хранения истории запросов
- Отслеживание прогресса обработки документов в реальном времени
- Автоматическое обновление страницы при обработке документов

## Требования

- Python 3.9+
- Ollama (локальная языковая модель)
- PostgreSQL
- Node.js 14+ (для фронтенда)
- Docker и Docker Compose (опционально)

## Установка и запуск

### Вариант 1: Запуск с помощью Docker Compose

1. Клонировать репозиторий
2. Создать файл `.env` на основе `.env.example`:
   ```bash
   cp .env.example .env
   ```
3. Отредактировать `.env` под свои нужды
4. Запустить с помощью Docker Compose:
   ```bash
   docker-compose up -d
   ```

### Вариант 2: Локальный запуск

#### Бэкенд

1. Создать и активировать виртуальное окружение:
   ```bash
   python -m venv venv
   source venv/bin/activate  # для Linux/macOS
   venv\Scripts\activate     # для Windows
   ```

2. Установить зависимости:
   ```bash
   pip install -r requirements.txt
   ```

3. Установить и запустить Ollama с нужной моделью:
   ```bash
   # Установить Ollama с официального сайта https://ollama.com/download
   # Запустить сервер Ollama
   ollama serve
   ```

4. Запустить Qdrant в Docker:
   ```bash
   docker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant
   ```

5. Запустить бэкенд:
   ```bash
   python -m app.main
   ```

#### Фронтенд

1. Перейти в директорию frontend:
   ```bash
   cd frontend
   ```
2. Установить зависимости:
   ```bash
   npm install
   ```
3. Запустить фронтенд:
   ```bash
   npm start
   ```

## Конфигурация

Основные настройки системы находятся в файле `.env`. Некоторые важные параметры:

- `EMBEDDING_MODEL`: модель для создания векторных представлений текста (по умолчанию: intfloat/multilingual-e5-base)
- `OLLAMA_MODEL`: модель Ollama для генерации ответов (по умолчанию: mistral:7b-instruct)
- `CHUNK_SIZE`: размер текстовых фрагментов в символах (по умолчанию: 1000)
- `CHUNK_OVERLAP`: перекрытие между фрагментами (по умолчанию: 300)
- `USE_HYBRID_SEARCH`: использовать гибридный поиск (по умолчанию: true)
- `USE_RERANKER`: использовать переранжирование (по умолчанию: true)
- `CHUNKING_MODE`: режим разделения на фрагменты (character, token, semantic, hierarchical)

## Требования к зависимостям

Для корректной работы системы убедитесь в следующей совместимости версий библиотек:

- huggingface_hub==0.14.1
- transformers==4.30.2
- sentence-transformers==2.2.2
- tokenizers==0.13.3

## Архитектура проекта

```
rag/
├── app/                  # Основной код бэкенда
│   ├── chunking/         # Модули для разделения текста на фрагменты
│   ├── retrieval/        # Компоненты для векторного поиска
│   ├── utils/            # Вспомогательные функции
│   ├── config.py         # Конфигурация приложения
│   ├── database.py       # Работа с базой данных
│   ├── logging_config.py # Настройка логирования
│   ├── main.py           # Основной файл FastAPI приложения
│   ├── models.py         # Модели базы данных (ORM)
│   ├── ollama_client.py  # Клиент для взаимодействия с Ollama
│   ├── rag.py            # Основной сервис RAG
│   └── schemas.py        # Pydantic модели для FastAPI
├── data/                 # Данные для приложения
├── frontend/             # React фронтенд
│   ├── public/           # Статические файлы
│   └── src/              # Исходный код фронтенда
├── indexes/              # Директория для хранения индексов
├── logs/                 # Логи приложения
├── scripts/              # Вспомогательные скрипты
├── tests/                # Тесты
├── .env.example          # Пример конфигурации окружения
├── Dockerfile            # Конфигурация Docker образа
├── docker-compose.yml    # Конфигурация Docker Compose
└── requirements.txt      # Python зависимости
```

## API Endpoints

### Аутентификация
- `POST /token` - получение токена доступа
- `POST /register` - регистрация нового пользователя

### Работа с документами
- `POST /documents/upload` - загрузка новых документов
- `GET /documents` - получение списка загруженных документов
- `DELETE /documents/{document_id}` - удаление документа
- `GET /documents/{document_id}` - получение информации о документе
- `POST /documents/reindex` - переиндексация документов
- `POST /index/clear` - очистка индекса

### Чат и генерация ответов
- `POST /ask` - запрос с использованием RAG
- `POST /direct-ask` - прямой запрос к модели без RAG
- `GET /chats` - получение истории чатов
- `DELETE /chats/clear` - очистка истории чатов

### Настройки модели
- `GET /model/settings` - получение настроек модели
- `POST /model/settings` - обновление настроек модели
- `GET /model/available` - проверка доступных моделей

### Системные эндпоинты
- `GET /api/healthcheck` - проверка состояния системы

## Как использовать

1. **Регистрация и вход**: зарегистрируйтесь и войдите в систему через фронтенд (http://localhost:3000)
2. **Загрузка документов**: загрузите документы через интерфейс загрузки
   - При загрузке укажите заголовок для документа
   - Система поддерживает различные форматы, включая PDF, DOCX, TXT, MD
3. **Мониторинг обработки**: отслеживайте прогресс обработки документов в реальном времени
   - Просматривайте текущие статусы документов: загрузка, извлечение текста, разбиение, создание эмбеддингов, индексация
   - Наблюдайте за общим индикатором прогресса для всех документов
   - Страница автоматически обновляется каждые 2 секунды во время обработки
4. **Задавание вопросов**: задавайте вопросы в чат-интерфейсе. Система найдет релевантную информацию в загруженных документах и сгенерирует ответ
5. **Настройка параметров**: при необходимости настройте параметры модели в разделе настроек

## Жизненный цикл обработки документов

Система отображает следующие статусы обработки документов:

1. **uploaded**: документ загружен, ожидает обработки (10% прогресса)
2. **processing**: извлечение текста из документа (25% прогресса)
3. **chunking**: разбиение текста на фрагменты (50% прогресса)
4. **embedding**: создание векторных представлений фрагментов (75% прогресса)
5. **indexing**: добавление векторов в индекс (90% прогресса)
6. **indexed**: документ успешно обработан и проиндексирован (100% прогресса)
7. **reindexing**: документ находится в процессе переиндексации
8. **error**: ошибка при обработке документа

## Используемые порты

- **8000**: FastAPI бэкенд
- **3000**: React фронтенд
- **6333**: Qdrant векторная база данных
- **11434**: Ollama сервер

## Разработка и тестирование

Проект включает тесты, которые можно запустить следующим образом:

```bash
# Запуск тестов бэкенда
pytest

# Запуск тестов фронтенда
cd frontend
npm test
```

## Известные проблемы и решения

1. **Проблема с SentenceTransformer**:
   В случае ошибки `cannot import name 'cached_download' from 'huggingface_hub'` необходимо установить совместимые версии библиотек:
   ```bash
   pip install huggingface_hub==0.14.1 transformers==4.30.2 sentence-transformers==2.2.2 tokenizers==0.13.3
   ```

2. **Проблема с LangChain**:
   Предупреждение о устаревшем импорте из `langchain.vectorstores`. В будущих версиях потребуется обновить импорт на `langchain_community.vectorstores`.

## Лицензия

[MIT](LICENSE) 