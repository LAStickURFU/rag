# RAG-система на русском языке

## Обзор проекта

Данный проект представляет собой полноценную систему Retrieval-Augmented Generation (RAG) с поддержкой русского языка. Система объединяет в себе современные подходы к поиску информации и генерации текста с использованием языковых моделей.

### Ключевые особенности

- **Гибридный поиск**: комбинация векторного поиска и BM25 для максимальной релевантности
- **Переранжирование результатов**: использование CrossEncoder для уточнения релевантности найденных документов
- **Адаптивный выбор контекста**: умная система определения оптимального количества документов для контекста
- **Отказоустойчивый чанкинг**: интеллектуальное разделение документов на смысловые фрагменты
- **Оценка качества**: комплексная система метрик для анализа эффективности RAG
- **Мультиязычная поддержка**: оптимизирована для работы с русским языком
- **Современный веб-интерфейс**: удобное взаимодействие с системой через браузер

## Технический стек

### Бэкенд
- **FastAPI**: веб-фреймворк для создания API
- **SQLAlchemy**: ORM для работы с базой данных
- **Sentence Transformers**: библиотека для работы с векторными представлениями текста
- **Transformers**: библиотека для работы с трансформерными моделями
- **Spacy**: библиотека для обработки естественного языка с русскоязычной моделью
- **Ollama**: интерфейс для запуска локальных языковых моделей
- **Qdrant**: векторная база данных для хранения эмбеддингов
- **RAGAS**: библиотека для оценки качества RAG-систем

### Фронтенд
- **React**: библиотека для построения пользовательских интерфейсов
- **Jest**: фреймворк для тестирования JavaScript-кода

### Хранение данных
- **PostgreSQL**: реляционная база данных для хранения метаданных
- **Qdrant**: векторная база данных для поиска релевантной информации

### Инфраструктура
- **Docker**: контейнеризация приложения
- **Docker Compose**: оркестрация контейнеров
- **Pytest**: фреймворк для тестирования Python-кода

## Архитектура системы

RAG-система состоит из следующих компонентов:

1. **Модуль чанкинга** (`app/chunking/`): разбивает документы на смысловые фрагменты подходящего размера.
2. **Модуль индексации** (`app/retrieval/`): преобразует текстовые фрагменты в векторные представления и сохраняет их в Qdrant.
3. **Модуль поиска**: находит релевантные фрагменты для заданного вопроса, используя гибридный подход.
4. **Модуль генерации**: формирует промпт на основе найденных фрагментов и отправляет его в языковую модель.
5. **Модуль оценки** (`app/evaluation/`): анализирует качество работы системы с использованием различных метрик.
6. **API** (`app/main.py`): предоставляет интерфейс для взаимодействия с системой.
7. **Веб-интерфейс** (`frontend/`): обеспечивает удобное пользовательское взаимодействие.

## Запуск проекта

### Предварительные требования

- Python 3.10+
- Docker и Docker Compose
- Git
- Node.js и npm (для разработки фронтенда)
- (Опционально) NVIDIA GPU с установленным CUDA для ускорения работы моделей
- Ollama (для локального запуска)

### Запуск через Docker

Самый простой способ запустить всю систему - использовать Docker Compose:

1. Клонируйте репозиторий:
   ```bash
   git clone <url-репозитория>
   cd rag
   ```

2. Создайте файл .env на основе .env.example:
   ```bash
   cp .env.example .env
   ```

3. (Опционально) Отредактируйте .env под свои нужды:
   ```
   # Настройки языковой модели
   OLLAMA_MODEL=mistral:7b-instruct
   OLLAMA_HOST=http://localhost:11434
   # ...другие настройки
   ```

4. Запустите сервисы через Docker Compose:
   ```bash
   docker-compose up -d
   ```
   
   Это запустит:
   - PostgreSQL (база данных)
   - Qdrant (векторная база данных)
   - FastAPI бэкенд
   - React фронтенд
   
   При первом запуске будут автоматически созданы необходимые таблицы в базе данных.

5. Проверьте, что все сервисы запущены:
   ```bash
   python check_services.py
   ```

6. Откройте веб-интерфейс по адресу [http://localhost:3000](http://localhost:3000)

#### Просмотр логов Docker-контейнеров

```bash
# Логи всех контейнеров
docker-compose logs

# Логи конкретного сервиса
docker-compose logs rag-api
```

#### Остановка и удаление контейнеров

```bash
# Остановка сервисов
docker-compose stop

# Остановка и удаление контейнеров
docker-compose down

# Остановка, удаление контейнеров и томов с данными
docker-compose down -v
```

### Локальный запуск для разработки

Для разработки и отладки удобнее запускать компоненты системы локально:

1. Установите Ollama:
   ```bash
   # macOS / Linux
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Windows
   # Скачайте установщик с https://ollama.com
   ```

2. Загрузите модель Mistral 7B:
   ```bash
   ollama pull mistral:7b-instruct
   ```

3. Установите PostgreSQL и запустите его:
   ```bash
   # Можно использовать Docker
   docker run -d --name postgres \
     -e POSTGRES_USER=postgres \
     -e POSTGRES_PASSWORD=mysecretpassword \
     -e POSTGRES_DB=postgres \
     -p 5432:5432 \
     postgres:15
   ```

4. Установите Qdrant и запустите его:
   ```bash
   # Через Docker
   docker run -d --name qdrant \
     -p 6333:6333 \
     -p 6334:6334 \
     -v $(pwd)/qdrant_data:/qdrant/storage \
     qdrant/qdrant
   ```

5. Создайте и активируйте виртуальное окружение Python:
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # macOS / Linux
   source venv/bin/activate
   ```

6. Установите зависимости Python:
   ```bash
   pip install -r requirements.txt
   ```

7. Создайте файл .env:
   ```bash
   cp .env.example .env
   ```
   
   При необходимости отредактируйте параметры подключения к базе данных и другие настройки.

8. Создайте пользователя и базу данных (если запускаете PostgreSQL локально):
   ```bash
   python create_pg_user_and_db.py
   ```

9. Запустите бэкенд:
   ```bash
   # Вариант 1: Через скрипт
   python -m scripts.run_app
   
   # Вариант 2: Напрямую через uvicorn
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

10. В отдельном терминале запустите фронтенд:
    ```bash
    cd frontend
    npm install
    npm start
    ```

11. Откройте веб-интерфейс по адресу [http://localhost:3000](http://localhost:3000)

#### Перестроение индекса

Если вам нужно перестроить векторный индекс:

```bash
python -m scripts.rebuild_index
```

#### Проверка состояния сервисов

```bash
python check_services.py
```

## Использование системы

### API-эндпоинты

- `POST /api/ask` - Задать вопрос RAG-системе
- `POST /api/documents` - Загрузить новые документы в систему
- `GET /api/documents` - Получить список загруженных документов
- `GET /api/evaluation/results` - Получить результаты оценки системы
- `POST /api/evaluation/run` - Запустить оценку качества RAG

#### Интерактивная документация API

FastAPI автоматически генерирует интерактивную документацию для всех эндпоинтов:

- Swagger UI: доступна по адресу `/docs` (например, http://localhost:8000/docs)
- ReDoc: доступна по адресу `/redoc` (например, http://localhost:8000/redoc)

С помощью этой документации вы можете:
- Просматривать все доступные эндпоинты и их параметры
- Тестировать API запросы прямо из браузера
- Изучать схемы запросов и ответов

### Оценка качества RAG

Система поддерживает комплексную оценку качества RAG с использованием библиотеки RAGAS:

```bash
# Запуск оценки с использованием встроенного датасета (sberquad или RuBQ)
python evaluate_rag.py --data sberquad

# Запуск оценки только извлечения (retrieval)
python evaluate_rag.py --data sberquad --retrieval-only

# Запуск оценки только генерации
python evaluate_rag.py --data sberquad --generation-only

# Просмотр доступных датасетов
python evaluate_rag.py --list-datasets

# Тестовый запуск оценки
python test_rag_eval.py --builtin --dataset sberquad
```

Доступные метрики:
- Faithfulness (достоверность) - насколько ответ соответствует предоставленному контексту
- Answer Relevancy (релевантность ответа) - насколько ответ соответствует вопросу
- Context Recall (полнота контекста) - насколько полно контекст покрывает информацию, необходимую для ответа
- Context Precision (точность контекста) - насколько точно контекст соответствует информационной потребности в вопросе

## Настройка и конфигурация

Система настраивается через файл .env. Основные параметры:

```
# Языковая модель
OLLAMA_MODEL=mistral:7b-instruct
OLLAMA_TIMEOUT=120

# Модели для embeddings и ранжирования
EMBEDDING_MODEL=intfloat/multilingual-e5-base
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Настройки чанкинга
CHUNK_SIZE=400
CHUNK_OVERLAP=100
LANGUAGE=russian
SPACY_MODEL=ru_core_news_md

# Настройки поиска
USE_HYBRID_SEARCH=True
USE_RERANKER=True
DENSE_WEIGHT=0.7
RERANKER_WEIGHT=0.5
USE_ADAPTIVE_K=True
MIN_K=2
MAX_K=10
```

### Настройка моделей

В системе используются несколько моделей машинного обучения:

1. **Языковая модель (LLM)** - используется для генерации ответов:
   - По умолчанию: `mistral:7b-instruct`
   - Запускается через Ollama
   - Можно заменить на другие модели (llama2, llama3, vicuna и т.д.)

2. **Модель эмбеддингов** - используется для векторизации текста:
   - По умолчанию: `intfloat/multilingual-e5-base`
   - Многоязычная модель с хорошей поддержкой русского языка
   - Альтернативы: `sentence-transformers/multilingual-mpnet-base-v2`, `intfloat/multilingual-e5-large`

3. **Модель ранжирования (CrossEncoder)** - используется для уточнения релевантности:
   - По умолчанию: `cross-encoder/ms-marco-MiniLM-L-6-v2`
   - Оптимизирована для ранжирования пар запрос-документ

## Структура проекта

```
rag/
├── app/                      # Основной код приложения
│   ├── chunking/             # Модули для разбиения документов на фрагменты
│   ├── retrieval/            # Компоненты для поиска релевантных документов
│   ├── evaluation/           # Инструменты для оценки качества RAG
│   ├── config.py             # Конфигурация приложения
│   ├── database.py           # Интеграция с базой данных
│   ├── main.py               # Основной файл FastAPI с эндпоинтами
│   ├── models.py             # Модели данных
│   ├── ollama_client.py      # Клиент для взаимодействия с Ollama
│   ├── rag.py                # Основная логика RAG
│   └── schemas.py            # Схемы Pydantic для валидации данных
├── frontend/                 # React-приложение для веб-интерфейса
├── scripts/                  # Вспомогательные скрипты
│   ├── rebuild_index.py      # Перестроение индекса
│   ├── run_app.py            # Запуск приложения
│   └── run_evaluation.py     # Запуск оценки качества
├── tests/                    # Тесты
├── logs/                     # Директория для логов
├── qdrant_data/              # Данные Qdrant
├── .env.example              # Пример конфигурации
├── docker-compose.yml        # Конфигурация Docker Compose
├── Dockerfile                # Dockerfile для сборки приложения
└── requirements.txt          # Зависимости Python
```

## Тестирование

Запуск всех тестов:
```bash
pytest
```

Запуск конкретных тестов:
```bash
pytest tests/test_rag_quality.py
```

## Устранение неполадок

### Проблемы с подключением к Ollama

Если система не может подключиться к Ollama, убедитесь, что:
1. Ollama запущен (`ollama serve`)
2. Нужная модель загружена (`ollama list`)
3. Правильно указан хост в .env (`OLLAMA_HOST=http://localhost:11434`)

### Проблемы с базой данных

Если есть проблемы с подключением к PostgreSQL:
1. Проверьте, что сервис запущен
2. Проверьте настройки подключения в .env
3. Выполните миграцию базы данных вручную:
   ```bash
   # Подключение к PostgreSQL
   psql -U postgres -h localhost
   ```